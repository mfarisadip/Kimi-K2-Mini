#\!/usr/bin/env python3
from vllm import LLM, SamplingParams
import torch

print("\nğŸš€ vLLM K2-Mini æµ‹è¯•\n")

model_path = "./k2-mini"

try:
    # æ£€æŸ¥GPUä¿¡æ¯
    print(f"GPUä¿¡æ¯:")
    print(f"  è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"  å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    print(f"\næ­£åœ¨ä½¿ç”¨vLLMåŠ è½½K2-Miniæ¨¡å‹...")
    
    # åˆ›å»ºLLMå®ä¾‹ï¼ˆä½¿ç”¨è¾ƒå°çš„GPUå†…å­˜åˆ©ç”¨ç‡ï¼‰
    llm = LLM(
        model=model_path,
        trust_remote_code=True,
        dtype="float16",
        gpu_memory_utilization=0.8,
        max_model_len=2048,  # é™åˆ¶åºåˆ—é•¿åº¦ä»¥èŠ‚çœå†…å­˜
        tensor_parallel_size=1,
        seed=42
    )
    
    print(f"âœ… vLLMæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=100,
        stop=["\n\n"]  # ç®€å•çš„åœæ­¢æ¡ä»¶
    )
    
    # æµ‹è¯•æç¤º
    prompts = [
        "äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
        "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    ]
    
    print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆæµ‹è¯•...")
    
    # æ‰¹é‡ç”Ÿæˆ
    outputs = llm.generate(prompts, sampling_params)
    
    print(f"\nç”Ÿæˆç»“æœ:")
    print(f"=" * 50)
    
    for i, output in enumerate(outputs):
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"\næç¤º {i+1}: {prompt}")
        print(f"ç”Ÿæˆ: {generated_text}")
        print(f"-" * 30)
    
    print(f"\nğŸ‰ vLLMæµ‹è¯•å®Œæˆï¼æ¨¡å‹è¿è¡Œæ­£å¸¸ï¼")
    
except Exception as e:
    print(f"\nâŒ vLLMé”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
    
    if "CUDA out of memory" in str(e):
        print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"1. å½“å‰MIGå®ä¾‹æ˜¾å­˜ä¸è¶³(10GB)")
        print(f"2. éœ€è¦å®Œæ•´H100 GPU (80GB)æ¥è¿è¡Œ32.5Bæ¨¡å‹")
        print(f"3. æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®")
