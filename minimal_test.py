import torch
import sys
import os

print('ğŸš€ K2-Mini æœ€å°åŒ–æµ‹è¯•')
print('='*50)

# æ£€æŸ¥GPU
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
else:
    print('âŒ æ²¡æœ‰æ‰¾åˆ°GPU')
    sys.exit(1)

# å°è¯•åŠ è½½æ¨¡å‹
model_path = 'k2-mini'
print(f'\nå°è¯•ä» {model_path} åŠ è½½æ¨¡å‹...')

try:
    # é¦–å…ˆåªåŠ è½½tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    print('âœ… Tokenizer åŠ è½½æˆåŠŸ')
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    import json
    with open(os.path.join(model_path, 'config.json')) as f:
        config = json.load(f)
    print(f'âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ')
    print(f'   æ¶æ„: {config.get("architectures", ["Unknown"])[0]}')
    
    # å°è¯•ä½¿ç”¨vLLMåŠ è½½ï¼ˆå¯èƒ½æ›´é€‚åˆè¿™ç§å¤§æ¨¡å‹ï¼‰
    print('\nå°è¯•ä½¿ç”¨vLLMåŠ è½½æ¨¡å‹...')
    try:
        from vllm import LLM, SamplingParams
        
        # ä½¿ç”¨è¾ƒå°çš„GPUå†…å­˜é™åˆ¶
        llm = LLM(
            model=model_path,
            trust_remote_code=True,
            dtype='float16',
            gpu_memory_utilization=0.7,
            max_model_len=2048,
            enforce_eager=True  # ç¦ç”¨CUDAå›¾ä»¥é¿å…æŸäº›é”™è¯¯
        )
        
        print('âœ… vLLMæ¨¡å‹åŠ è½½æˆåŠŸ\!')
        
        # ç®€å•æµ‹è¯•
        sampling_params = SamplingParams(temperature=0.7, max_tokens=50)
        prompt = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
        
        print(f'\næµ‹è¯•ç”Ÿæˆ...')
        print(f'è¾“å…¥: {prompt}')
        
        outputs = llm.generate([prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        print(f'è¾“å‡º: {generated_text}')
        print('\nâœ… K2-Mini è¿è¡ŒæˆåŠŸ\!')
        
    except Exception as e:
        print(f'\nâŒ vLLMåŠ è½½å¤±è´¥: {e}')
        print('\nå°è¯•ä½¿ç”¨transformersç›´æ¥åŠ è½½...')
        
        # å°è¯•ä¸ä½¿ç”¨device_map
        from transformers import AutoModelForCausalLM
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map=None  # ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        )
        model = model.to('cuda')
        print('âœ… Transformersæ¨¡å‹åŠ è½½æˆåŠŸ\!')
        
except Exception as e:
    print(f'\nâŒ é”™è¯¯: {e}')
    import traceback
    traceback.print_exc()
