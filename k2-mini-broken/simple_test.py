import torch
print("\næµ‹è¯•K2-Miniæ¨¡å‹åŠ è½½...\n")

try:
    # ç›´æ¥å¯¼å…¥æ¨¡å‹ç±»
    import sys
    sys.path.insert(0, '.')
    from modeling_deepseek import DeepseekV3ForCausalLM
    from transformers import AutoConfig
    
    # åŠ è½½é…ç½®
    config = AutoConfig.from_pretrained('.', trust_remote_code=True)
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   å±‚æ•°: {config.num_hidden_layers}")
    print(f"   ä¸“å®¶æ•°: {config.n_routed_experts}")
    
    # æµ‹è¯•æ¨¡å‹ç»“æ„
    print("\næµ‹è¯•æ¨¡å‹ç»“æ„åˆ›å»º...")
    with torch.device('meta'):
        model = DeepseekV3ForCausalLM(config)
    print("âœ… æ¨¡å‹ç»“æ„éªŒè¯æˆåŠŸ")
    
    print("\nğŸ’¡ æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œå¯ä»¥åŠ è½½ï¼")
    print("\næ³¨æ„ï¼šå®Œæ•´åŠ è½½éœ€è¦çº¦40GBæ˜¾å­˜")
    print("å½“å‰MIGå®ä¾‹åªæœ‰10GBï¼Œéœ€è¦å®Œæ•´çš„GPUæ¥è¿è¡Œ")
    
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
