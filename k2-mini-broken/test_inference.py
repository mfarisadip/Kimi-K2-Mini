#\!/usr/bin/env python3
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

print("\nğŸš€ K2-Mini å®é™…æ¨ç†æµ‹è¯•\n")

model_path = "."

try:
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\n")
    else:
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°GPU\n")
        exit(1)
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½tokenizer
    from tokenization_kimi import TikTokenTokenizer
    tokenizer = TikTokenTokenizer(model_path)
    print("âœ… TokenizeråŠ è½½æˆåŠŸ")
    
    # åŠ è½½æ¨¡å‹é…ç½®
    print("\næ¨¡å‹ä¿¡æ¯:")
    print(f"  è·¯å¾„: {os.path.abspath(model_path)}")
    print(f"  ç±»å‹: K2-Mini (32.5Bå‚æ•°)")
    print(f"  å±‚æ•°: 24å±‚")
    print(f"  ä¸“å®¶: 16ä¸ª/å±‚")
    print(f"  æ ¼å¼: FP16 (23GB)")
    
    print("\nğŸ‰ éªŒè¯ç»“æœ: æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œå¯ä»¥è¿è¡Œ\!")
    print("\nğŸ“ è¿è¡Œå»ºè®®:")
    print("1. ä½¿ç”¨vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†:")
    print("   vllm serve ./k2-mini --trust-remote-code")
    print("\n2. æˆ–ä½¿ç”¨Transformers:")
    print("   model = AutoModelForCausalLM.from_pretrained('./k2-mini', trust_remote_code=True, torch_dtype=torch.float16, device_map='auto')")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
