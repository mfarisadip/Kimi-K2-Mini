#\!/usr/bin/env python3
import os
import json

print('\nğŸ¯ K2-Mini é¡¹ç›®æœ€ç»ˆçŠ¶æ€æ€»ç»“')
print('='*60)

# æ£€æŸ¥å„ä¸ªç‰ˆæœ¬
versions = {
    'k2-mini': 'ä¸»ç‰ˆæœ¬ï¼ˆä»å¤‡ä»½æ¢å¤ï¼‰',
    'k2-mini-broken': 'åŸå§‹æœ‰é—®é¢˜çš„ç‰ˆæœ¬',
    'k2-mini-fast': 'å¿«é€Ÿè½¬æ¢ç‰ˆæœ¬',
    'k2-mini-backup-weights': 'å¤‡ä»½æƒé‡'
}

print('\nğŸ“ å¯ç”¨ç‰ˆæœ¬:')
for path, desc in versions.items():
    if os.path.exists(path):
        size = sum(os.path.getsize(os.path.join(path, f)) 
                  for f in os.listdir(path) 
                  if f.endswith('.safetensors')) / 1024**3
        print(f'  âœ“ {path}: {desc} ({size:.1f} GB)')
    else:
        print(f'  âœ— {path}: ä¸å­˜åœ¨')

# æ£€æŸ¥ä¸»ç‰ˆæœ¬é…ç½®
print('\nğŸ”§ ä¸»ç‰ˆæœ¬(k2-mini)é…ç½®:')
with open('k2-mini/config.json') as f:
    config = json.load(f)
    print(f'  å±‚æ•°: {config.get("num_hidden_layers")}')
    print(f'  è·¯ç”±ä¸“å®¶æ•°: {config.get("n_routed_experts")}')
    print(f'  å…±äº«ä¸“å®¶æ•°: {config.get("n_shared_experts")}')
    print(f'  é‡åŒ–æ–¹æ³•: {config.get("quantization_config", {}).get("quant_method", "æ— ")}')

print('\nğŸ“Š å‘ç°çš„é—®é¢˜:')
print('  1. å…±äº«ä¸“å®¶æƒé‡ç¼ºå¤±ï¼ˆ72ä¸ªï¼‰')
print('  2. ä¸“å®¶é—¨æ§æƒé‡ç»´åº¦ä¸åŒ¹é…ï¼ˆ384 vs 16ï¼‰')
print('  3. DynamicCache APIå…¼å®¹æ€§é—®é¢˜')
print('  4. FP8é‡åŒ–æƒé‡ä¸FP16ä¸å…¼å®¹')

print('\nâœ… å·²å®Œæˆçš„ä¿®å¤:')
print('  1. æ¢å¤å¤‡ä»½æƒé‡')
print('  2. ç¦ç”¨å…±äº«ä¸“å®¶ï¼ˆn_shared_experts=0ï¼‰')
print('  3. ä¿®å¤meta tensorï¼ˆè½¬æ¢ä¸ºFP16ï¼‰')
print('  4. æ¨¡å‹å¯ä»¥åŠ è½½ï¼ˆä½¿ç”¨40.6GBæ˜¾å­˜ï¼‰')

print('\nğŸ’¡ å»ºè®®çš„ä¸‹ä¸€æ­¥:')
print('  1. ä¿®å¤modeling_deepseek.pyä¸­çš„ç¼“å­˜å…¼å®¹æ€§')
print('  2. æˆ–ä½¿ç”¨vLLM/å…¶ä»–æ¨ç†æ¡†æ¶')
print('  3. æˆ–é‡æ–°å®Œæ•´è½¬æ¢ï¼ˆä½¿ç”¨ä¿®æ­£åçš„è„šæœ¬ï¼‰')

print('\nğŸš€ K2-Minié¡¹ç›®çŠ¶æ€: éƒ¨åˆ†æˆåŠŸ')
print('   - æ¨¡å‹ç»“æ„ âœ“')
print('   - æƒé‡åŠ è½½ âœ“')
print('   - æ¨ç†ç”Ÿæˆ âœ— (éœ€è¦ä¿®å¤)')
