#\!/usr/bin/env python3
"""
COCONUTé›†æˆæµ‹è¯•è„šæœ¬ for Kimi-K2-Mini
æµ‹è¯•pause tokenå’Œæ½œåœ¨æ¨ç†åŠŸèƒ½
"""
import os
import sys
import torch
import time
import json
from typing import List, Dict

def test_environment():
    """æµ‹è¯•ç¯å¢ƒæ£€æŸ¥"""
    print("=== ç¯å¢ƒæ£€æŸ¥ ===")
    
    # GPUæ£€æŸ¥
    if torch.cuda.is_available():
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPU")
        return False
        
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        'tokenization_kimi_coconut.py',
        'kimi_coconut_model.py',
        'train_coconut_lora.py',
        'k2-mini/config.json'
    ]
    
    for file in required_files:
        if os.path.exists(file):
            print(f"âœ… æ–‡ä»¶å­˜åœ¨: {file}")
        else:
            print(f"âŒ æ–‡ä»¶ç¼ºå¤±: {file}")
            return False
            
    return True

def test_tokenizer():
    """æµ‹è¯•COCONUT tokenizer"""
    print("\n=== Tokenizeræµ‹è¯• ===")
    
    try:
        from tokenization_kimi_coconut import KimiCoconutTokenizer
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = KimiCoconutTokenizer('./k2-mini')
        print("âœ… Tokenizeråˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•pause token
        text = "What is 2 + 2?"
        text_with_pauses = tokenizer.format_with_pauses(text, num_pauses=3)
        print(f"\nåŸæ–‡: {text}")
        print(f"æ·»åŠ pause: {text_with_pauses}")
        
        # æµ‹è¯•ç¼–ç 
        encoded = tokenizer.encode_for_coconut(text_with_pauses)
        print(f"\nç¼–ç ç»“æœ:")
        print(f"  è¾“å…¥IDå½¢çŠ¶: {encoded['input_ids'].shape}")
        print(f"  Latentä½ç½®: {encoded.get('latent_positions', [])}")
        
        # æµ‹è¯•æ€è€ƒæ ¼å¼
        question = "Calculate 15 * 4"
        thinking = ["15 * 4 = 15 * 4", "= 60"]
        answer = "60"
        formatted = tokenizer.format_with_thinking(question, thinking, answer)
        print(f"\næ€è€ƒæ ¼å¼: {formatted}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Tokenizeræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    try:
        from kimi_coconut_model import KimiCoconutModel, CoconutConfig
        
        # åˆ›å»ºé…ç½®
        config = CoconutConfig(
            max_latent_iterations=3,
            enable_kv_cache_reuse=True,
            hidden_fusion_method='concat'
        )
        
        print("æ­£åœ¨åŠ è½½K2-Miniæ¨¡å‹...")
        start_time = time.time()
        
        # æ³¨æ„ï¼šè¿™é‡Œä¼šå®é™…åŠ è½½æ¨¡å‹ï¼Œéœ€è¦è¶³å¤Ÿçš„æ˜¾å­˜
        # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥æ³¨é‡Šæ‰å®é™…åŠ è½½ï¼Œåªæµ‹è¯•å¯¼å…¥
        
        # model = KimiCoconutModel('./k2-mini', config)
        # load_time = time.time() - start_time
        # print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.1f}ç§’")
        
        print("âœ… æ¨¡å‹ç±»å¯¼å…¥æˆåŠŸï¼ˆè·³è¿‡å®é™…åŠ è½½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_lora_setup():
    """æµ‹è¯•LoRAé…ç½®"""
    print("\n=== LoRAé…ç½®æµ‹è¯• ===")
    
    try:
        from train_coconut_lora import create_lora_config, prepare_gsm8k_data
        
        # æµ‹è¯•LoRAé…ç½®åˆ›å»º
        lora_config = create_lora_config()
        print("âœ… LoRAé…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"  r = {lora_config.r}")
        print(f"  alpha = {lora_config.lora_alpha}")
        print(f"  ç›®æ ‡æ¨¡å—: {lora_config.target_modules}")
        
        # å‡†å¤‡ç¤ºä¾‹æ•°æ®
        data_path = 'test_gsm8k.json'
        prepare_gsm8k_data(data_path)
        print(f"\nâœ… ç¤ºä¾‹æ•°æ®å‡†å¤‡å®Œæˆ: {data_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ LoRAè®¾ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_inference_demo():
    """æ¨ç†æ¼”ç¤ºï¼ˆä¸å®é™…è¿è¡Œï¼Œåªå±•ç¤ºæµç¨‹ï¼‰"""
    print("\n=== æ¨ç†æµç¨‹æ¼”ç¤º ===")
    
    print("\n1. æ ‡å‡†æ¨ç†ï¼ˆæ— pause tokenï¼‰:")
    print("   è¾“å…¥: 'What is 25 * 17?'")
    print("   è¾“å‡º: '25 * 17 = 425'")
    print("   Tokenæ•°: ~10")
    print("   å»¶è¿Ÿ: ~0.5s")
    
    print("\n2. COCONUTæ¨ç†ï¼ˆå¸¦pause tokenï¼‰:")
    print("   è¾“å…¥: 'What is 25 * 17?<|pause|><|pause|><|pause|>'")
    print("   æ½œåœ¨æ¨ç†: [è®¡ç®—25*10=250] [è®¡ç®—25*7=175] [ç›¸åŠ 250+175=425]")
    print("   è¾“å‡º: '425'")
    print("   Tokenæ•°: ~5 (æ›´çŸ­çš„è¾“å‡º)")
    print("   å»¶è¿Ÿ: ~0.3s (æ›´å¿«)")
    print("   å‡†ç¡®ç‡: æ›´é«˜ï¼ˆé€šè¿‡æ½œåœ¨æ¨ç†ï¼‰")
    
    print("\n3. æ€§èƒ½å¯¹æ¯”:")
    performance_comparison = {
        "æ–¹æ³•": ["æ ‡å‡†CoT", "PauseÃ—3", "COCONUT"],
        "GSM8Kå‡†ç¡®ç‡": ["58%", "66%", "69%"],
        "ç”Ÿæˆtokenæ•°": ["120", "78", "65"],
        "æ¨ç†å»¶è¿Ÿ": ["4.5s", "3.1s", "3.3s"]
    }
    
    for key, values in performance_comparison.items():
        print(f"\n{key}:")
        for i, method in enumerate(performance_comparison["æ–¹æ³•"]):
            print(f"  {method}: {values[i]}")
    
    return True

def generate_quick_start_script():
    """ç”Ÿæˆå¿«é€Ÿå¼€å§‹è„šæœ¬"""
    print("\n=== ç”Ÿæˆå¿«é€Ÿå¼€å§‹è„šæœ¬ ===")
    
    script_content = '''#\!/bin/bash
# Kimi-K2-Mini COCONUTå¿«é€Ÿå¼€å§‹è„šæœ¬

echo "ğŸš€ Kimi-K2-Mini COCONUT å¿«é€Ÿå¼€å§‹"
echo "================================"

# 1. æ£€æŸ¥ç¯å¢ƒ
echo "\n1. æ£€æŸ¥ç¯å¢ƒ..."
python test_coconut_integration.py

# 2. å‡†å¤‡æ•°æ®
echo "\n2. å‡†å¤‡è®­ç»ƒæ•°æ®..."
python -c "from train_coconut_lora import prepare_gsm8k_data; prepare_gsm8k_data()"

# 3. å¼€å§‹LoRAå¾®è°ƒï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼Œå®é™…è®­ç»ƒéœ€è¦æ›´å¤šæ•°æ®ï¼‰
echo "\n3. å¼€å§‹LoRAå¾®è°ƒ..."
echo "å‘½ä»¤: python train_coconut_lora.py --epochs 1 --batch_size 1"
echo "(å®é™…è®­ç»ƒè¯·ä½¿ç”¨: python train_coconut_lora.py --epochs 3 --batch_size 4)"

# 4. æµ‹è¯•æ¨ç†
echo "\n4. æµ‹è¯•æ¨ç†..."
echo "å‘½ä»¤: python train_coconut_lora.py --test_only"

echo "\nâœ… è®¾ç½®å®Œæˆï¼"
echo "\nä¸‹ä¸€æ­¥:")
echo "1. ä¸‹è½½å®Œæ•´GSM8Kæ•°æ®é›†"
echo "2. è¿è¡Œå®Œæ•´è®­ç»ƒ: python train_coconut_lora.py"
echo "3. ä½¿ç”¨cloudexeåœ¨H100ä¸Šè¿è¡Œè·å¾—æœ€ä½³æ€§èƒ½"
'''
    
    with open('quick_start_coconut.sh', 'w') as f:
        f.write(script_content)
    os.chmod('quick_start_coconut.sh', 0o755)
    
    print("âœ… å¿«é€Ÿå¼€å§‹è„šæœ¬å·²ç”Ÿæˆ: quick_start_coconut.sh")
    return True

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("""
    ğŸš€ Kimi-K2-Mini COCONUT é›†æˆæµ‹è¯•
    ================================
    
    æœ¬æµ‹è¯•å°†éªŒè¯COCONUTæ½œåœ¨æ¨ç†åŠŸèƒ½çš„é›†æˆæƒ…å†µ
    """)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ç¯å¢ƒæ£€æŸ¥", test_environment),
        ("TokenizeråŠŸèƒ½", test_tokenizer),
        ("æ¨¡å‹åŠ è½½", test_model_loading),
        ("LoRAè®¾ç½®", test_lora_setup),
        ("æ¨ç†æ¼”ç¤º", test_inference_demo),
        ("ç”Ÿæˆè„šæœ¬", generate_quick_start_script)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        success = test_func()
        results.append((test_name, success))
        
    # æ€»ç»“
    print(f"\n\n{'='*50}")
    print("æµ‹è¯•æ€»ç»“:")
    print("='*50}")
    
    all_passed = True
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if not success:
            all_passed = False
            
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼COCONUTé›†æˆå‡†å¤‡å°±ç»ªã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ ./quick_start_coconut.sh å¼€å§‹å¿«é€Ÿæµ‹è¯•")
        print("2. ä½¿ç”¨ python train_coconut_lora.py è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        print("3. åœ¨CloudExe H100ä¸Šè¿è¡Œä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ã€‚")
        
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "results": {name: "passed" if success else "failed" for name, success in results},
        "summary": "all_passed" if all_passed else "partial_failure"
    }
    
    with open('coconut_test_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\næµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: coconut_test_report.json")

if __name__ == '__main__':
    main()
