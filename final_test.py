#\!/usr/bin/env python3
import torch
import os
import sys

print("\nğŸ¯ K2-Mini æœ€ç»ˆéªŒè¯\n")

model_path = "./k2-mini"

try:
    # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶
    required_files = {
        'config.json': 'é…ç½®æ–‡ä»¶',
        'tiktoken.model': 'Tokenizeræ¨¡å‹',
        'model.safetensors.index.json': 'æ¨¡å‹ç´¢å¼•',
        'modeling_deepseek.py': 'æ¨¡å‹ä»£ç ',
        'tokenization_kimi.py': 'Tokenizerä»£ç '
    }
    
    print("ğŸ“ æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥:")
    all_present = True
    for file, desc in required_files.items():
        path = os.path.join(model_path, file)
        exists = os.path.exists(path)
        print(f"   {desc}: {'âœ…' if exists else 'âŒ'}")
        if not exists:
            all_present = False
    
    # æ£€æŸ¥æ¨¡å‹åˆ†ç‰‡
    model_files = [f for f in os.listdir(model_path) if f.startswith('model-') and f.endswith('.safetensors')]
    print(f"\nğŸ“¦ æ¨¡å‹æƒé‡æ–‡ä»¶: {len(model_files)} ä¸ª")
    total_size = sum(os.path.getsize(os.path.join(model_path, f)) for f in model_files) / 1024**3
    print(f"   æ€»å¤§å°: {total_size:.1f} GB")
    
    if not all_present:
        print("\nâŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶")
        exit(1)
    
    # åŠ è½½é…ç½®éªŒè¯
    import json
    with open(os.path.join(model_path, 'config.json'), 'r') as f:
        config = json.load(f)
    
    print(f"\nğŸ”§ æ¨¡å‹é…ç½®:")
    print(f"   æ¶æ„: {config.get('architectures', ['æœªçŸ¥'])[0]}")
    print(f"   å±‚æ•°: {config.get('num_hidden_layers', 'æœªçŸ¥')}")
    print(f"   ä¸“å®¶æ•°: {config.get('n_routed_experts', 'æœªçŸ¥')}/å±‚")
    print(f"   éšè—ç»´åº¦: {config.get('hidden_size', 'æœªçŸ¥')}")
    print(f"   å‚æ•°é‡: ~32.5B")
    
    # GPUæ£€æŸ¥
    print(f"\nğŸ’» ç¡¬ä»¶ç¯å¢ƒ:")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   âš ï¸  æ³¨æ„: å½“å‰MIGå®ä¾‹åªæœ‰10GBï¼Œå®Œæ•´åŠ è½½éœ€è¦~40GB")
    else:
        print(f"   âŒ æœªæ£€æµ‹åˆ°GPU")
    
    print(f"\nâœ… éªŒè¯ç»“æœ: K2-Miniæ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼")
    print(f"\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print(f"1. åœ¨å®Œæ•´GPUä¸Šä½¿ç”¨TransformersåŠ è½½:")
    print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f"   model = AutoModelForCausalLM.from_pretrained('{os.path.abspath(model_path)}', trust_remote_code=True, torch_dtype=torch.float16)")
    print(f"\n2. æˆ–ä½¿ç”¨vLLMæœåŠ¡:")
    print(f"   vllm serve {os.path.abspath(model_path)} --trust-remote-code")
    
    print(f"\nğŸ‰ K2-Mini (32.5B) å·²å‡†å¤‡å°±ç»ªï¼")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
