#\!/usr/bin/env python3

def main():
    from vllm import LLM, SamplingParams
    import torch
    import time
    
    print('ğŸš€ CloudExe H100 ä¸Šè¿è¡Œ K2-Mini æµ‹è¯•')
    print('=====================================')
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
    print(f'åˆå§‹æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB')
    
    model_path = '/root/Kimi-K2-Mini/k2-mini'
    
    try:
        print(f'\næ­£åœ¨åŠ è½½ K2-Mini æ¨¡å‹...')
        start_time = time.time()
        
        # ä½¿ç”¨å®Œæ•´GPUèµ„æºé…ç½®
        llm = LLM(
            model=model_path,
            trust_remote_code=True,
            dtype='float16',
            gpu_memory_utilization=0.85,  # ä½¿ç”¨85%æ˜¾å­˜
            max_model_len=4096,           # è¾ƒé•¿åºåˆ—
            tensor_parallel_size=1,
            disable_custom_all_reduce=True,
            quantization=None,
            seed=42
        )
        
        load_time = time.time() - start_time
        print(f'âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.1f}ç§’')
        print(f'åŠ è½½åæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB')
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=150,
            stop=['\n\n', '\né—®ï¼š', '\nç­”ï¼š']
        )
        
        # æµ‹è¯•æç¤º
        prompts = [
            'äººå·¥æ™ºèƒ½çš„å‘å±•å†å²å¯ä»¥è¿½æº¯åˆ°',
            'æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„ä¸»è¦åŒºåˆ«æ˜¯',
            'è¯·è§£é‡Šä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼š'
        ]
        
        print(f'\nğŸ“ å¼€å§‹ç”Ÿæˆæµ‹è¯•...')
        
        for i, prompt in enumerate(prompts, 1):
            print(f'\n--- æµ‹è¯• {i} ---')
            print(f'è¾“å…¥: {prompt}')
            
            start_time = time.time()
            outputs = llm.generate([prompt], sampling_params)
            gen_time = time.time() - start_time
            
            output = outputs[0]
            generated_text = output.outputs[0].text
            tokens_generated = len(output.outputs[0].token_ids)
            
            print(f'è¾“å‡º: {generated_text}')
            print(f'ç”Ÿæˆé€Ÿåº¦: {tokens_generated/gen_time:.1f} tokens/ç§’')
            print(f'è€—æ—¶: {gen_time:.2f}ç§’')
        
        print(f'\nğŸ‰ K2-Mini åœ¨ CloudExe H100 ä¸Šè¿è¡ŒæˆåŠŸï¼')
        print(f'\nğŸ“Š æ€§èƒ½æ€»ç»“:')
        print(f'  æ¨¡å‹: K2-Mini (32.5Bå‚æ•°)')
        print(f'  GPU: H100 80GB') 
        print(f'  æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB')
        print(f'  åŠ è½½æ—¶é—´: {load_time:.1f}ç§’')
        
    except Exception as e:
        print(f'\nâŒ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
