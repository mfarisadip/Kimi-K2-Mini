#\!/usr/bin/env python3
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import gc

print("\nğŸš€ K2-Mini æ¨ç†æµ‹è¯•\n")

model_path = "./k2-mini"

try:
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°GPU")
        exit(1)
    
    device = torch.device("cuda")
    print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    print(f"   åˆå§‹æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**3:.1f} GB")
    
    # åŠ è½½tokenizer
    print("\næ­£åœ¨åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    print("âœ… TokenizeråŠ è½½æˆåŠŸ")
    
    # åŠ è½½æ¨¡å‹
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰...")
    start_time = time.time()
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )
    
    load_time = time.time() - start_time
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.1f}ç§’")
    print(f"   æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f} GB")
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    model.eval()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("\nğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    test_prompts = [
        "äººå·¥æ™ºèƒ½çš„å‘å±•å†å²å¯ä»¥è¿½æº¯åˆ°",
        "Pythonç¼–ç¨‹è¯­è¨€çš„ä¸»è¦ç‰¹ç‚¹æ˜¯",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\næµ‹è¯• {i}: {prompt}")
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # ç”Ÿæˆ
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        gen_time = time.time() - start_time
        
        # è§£ç è¾“å‡º
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        new_tokens = outputs[0].shape[0] - inputs['input_ids'].shape[1]
        
        print(f"ç”Ÿæˆå†…å®¹: {generated_text}")
        print(f"ç”Ÿæˆé€Ÿåº¦: {new_tokens/gen_time:.1f} tokens/ç§’")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
    print(f"   æ¨¡å‹å¤§å°: 32.5Bå‚æ•°")
    print(f"   æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f} GB")
    print(f"   æ¨ç†æ­£å¸¸: æ˜¯")
    print(f"\nğŸ‰ K2-Miniæ¨¡å‹éªŒè¯æˆåŠŸï¼å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
    
finally:
    # æ¸…ç†
    if 'model' in locals():
        del model
    gc.collect()
    torch.cuda.empty_cache()
